{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Different Deep Learning Packages\n",
    "\n",
    "\n",
    "### Torch\n",
    " - Lua\n",
    " \n",
    "### Caffe\n",
    " - very good for vision research\n",
    " - C++\n",
    "\n",
    "\n",
    "## Theano, Tensorflow\n",
    "- similarities and differences ... pros and cons\n",
    " \n",
    "\n",
    "### Pylearn2\n",
    " - more research oriented .. and you can see examples of advanced models\n",
    " - if you want to work with generative models\n",
    " - not continued any more .. but its good to study the code and take ideas of it.\n",
    " \n",
    "### Keras\n",
    " - Simplified and modular API structure.\n",
    " - Supports both Theano and Tensorflow backend .. so you can juggle between both and get the best of both worlds.. you are also welcome to develop support for new backends.\n",
    " - Can be used to create complex models by customizing any part \n",
    "   - custom layer\n",
    "   - custom loss, metrics\n",
    "   - custom activation function\n",
    "   - Functional models (can be used to build complex models other than sequential... also for Multiple Prediction Models)\n",
    " - Can be hooked into existing Theano/Tenserflow code\n",
    "   - Can be directly hooked up to Theano code .. for tensorflow you need to take care of using the same session (follow the bellow link)\n",
    "   - https://blog.keras.io/keras-as-a-simplified-interface-to-tensorflow-tutorial.html\n",
    " \n",
    " Documentation: https://keras.io/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_______________________________________________________________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backends \n",
    "\n",
    "config file ~/.keras/keras.json\n",
    "\n",
    "\n",
    " - \\$ KERAS_BACKEND=theano      python script.py\n",
    " - \\$ KERAS_BACKEND=tenserflow  python script.py\n",
    " \n",
    "Source Code: keras/backend/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "if K.backend() == 'theano':\n",
    "    print '\\nWe are using THEANO backend'\n",
    "else:\n",
    "    print '\\nWe are using TENSORFLOW backend'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_______________________________________________________________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tricks of setting up Neural Networks and Deep Learning\n",
    "\n",
    "Even though NNs are theoretically universal function approximator, it shouldn't be used as black box. For one the number of hidden units exponentially decreases with number of layers for the same capacity (what is capacity?) of the network. But training networks with more layers is slower .. and the set of tricks that allows to speed up the training is called Deep Learning. \n",
    "\n",
    "Some Tricks:\n",
    " - input representation\n",
    " - parameter initialization\n",
    " - target values\n",
    " - choice of learning rates\n",
    " - choice of nonlinearity (Activation functions)\n",
    " - SGD and other 2nd order algorithms to optimize - learning rate adaptation(Momentum)\n",
    " \n",
    "Fast minimization vs Good Generalization\n",
    " - Early Stopping\n",
    " - Weight Decay\n",
    " - Hyper-parameter Search\n",
    " - Validation Error\n",
    " - Dropout\n",
    " \n",
    "Network Models\n",
    " - Different Network architectures are suited to different problem domains\n",
    " - Multi-Task Learning/Prediction\n",
    " - Idea of Scaling and Centering ... not just to inputs but to all factors of NN .. most famous is Batch-Normalization.\n",
    "\n",
    "\n",
    "This is just the some of the techniques.. there are a lot more. Here I will show how you can employ and sometimes implement almost of the techniques with Keras. \n",
    "\n",
    "\n",
    "\n",
    "### Word of caution\n",
    "\n",
    " - Not all techniques gel with each other .. as you play more with these architectures .. you will grow more intuitions which ones to pick and for what situations \n",
    " - Also as these architectures are fairly powerful .. even with quick setting up you may good results.. but they might not be near best one. Much fiddling is needed to get the best of results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_______________________________________________________________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Activation, Input\n",
    "from keras.optimizers import SGD, Adam, RMSprop\n",
    "from keras.utils import np_utils\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   ## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "nb_classes = 10\n",
    "nb_epoch = 20\n",
    "\n",
    "# Loading Data\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print X_train.shape, y_train.shape\n",
    "print X_test.shape, y_test.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Checking the data\n",
    "\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "_len = 4\n",
    "nos =  np.random.randint(X_train.shape[0], size=_len)\n",
    "\n",
    "fig, axs = plt.subplots(1,_len)\n",
    "for idx in range(_len):\n",
    "    axs[idx].set_yticklabels([])\n",
    "    axs[idx].set_xticklabels([])\n",
    "    axs[idx].imshow(X_train[nos[idx]].reshape(28,28), cmap='gray')\n",
    "    axs[idx].set_title('{0}'.format(y_train[nos[idx]]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Pre-processing the data\n",
    "\n",
    "X_train = X_train.reshape(60000, 784)\n",
    "X_test = X_test.reshape(10000, 784)\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "Y_train = np_utils.to_categorical(y_train, nb_classes)\n",
    "Y_test = np_utils.to_categorical(y_test, nb_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print X_train.shape, Y_train.shape \n",
    "print X_test.shape, Y_test.shape, \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Layers:\n",
    " keras/layers/core.py ... and others in the folder\n",
    " \n",
    " ### Initialization\n",
    " init='glorot_uniform'  ... default initialization\n",
    " \n",
    " ### Regularization\n",
    " - W_regularizer\n",
    " - b_regularizer\n",
    " - activity_regularizer\n",
    " - W_constraint\n",
    " - b_constraint\n",
    " \n",
    " - Dropout\n",
    " - BatchNormalization\n",
    " \n",
    " ### Activation\n",
    " keras/activations.py \n",
    " ... why RELU?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Some alternate way to create the model:\n",
    "\n",
    "inp = Input(batch_shape=(None, 784))\n",
    "dense1 = Dense(512)\n",
    "act1 = Activation('relu')\n",
    "drop1 = Dropout(0.2)\n",
    "dense2 = Dense(512, activation='relu')\n",
    "drop2 = Dropout(0.2)\n",
    "dense3 = Dense(10)\n",
    "act2 = Activation('softmax')\n",
    "\n",
    "out = act2(dense3(drop2(dense2(drop1(act1(dense1(inp)))))))\n",
    "           \n",
    "model = Model(inp, out)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 3-Layered MLP network\n",
    "\n",
    "out = dense1(inp)\n",
    "out = act1(out)\n",
    "out = drop1(out)\n",
    "out = dense2(out)\n",
    "out = drop2(out)\n",
    "out = dense3(out)\n",
    "out = act2(out)\n",
    "           \n",
    "model = Model(inp, out)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Some alternate way to create the model:\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(512, input_shape=(784,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(10))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from IPython.display import SVG\n",
    "from keras.utils.visualize_util import model_to_dot\n",
    "\n",
    "SVG(model_to_dot(model, show_shapes=True).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss\n",
    "\n",
    "keras/objectives.py\n",
    "\n",
    "GoTo losses:\n",
    " - mean squared error (regression)\n",
    " - catagorical crossentropy (classification)\n",
    " - binary crossentropy\n",
    " - KL divergence (distance between distributions)\n",
    "\n",
    "Custom Losses:\n",
    " - Variational Loss\n",
    " - Smoothness penelty\n",
    " - How To:\n",
    "   - Write and verify your own objective functions before hooking up (my answer .. do give points up if you find it useful !!!) \n",
    "     - http://stackoverflow.com/questions/33859864/how-to-create-custom-objective-function-in-keras/40622302#40622302\n",
    "   - How to pass extra parameter other than y_pred and y_true\n",
    "     - https://github.com/fchollet/keras/issues/2121\n",
    " \n",
    " \n",
    "### Optimizer\n",
    "\n",
    "keras/optimizers.py\n",
    "       - 'sgd': SGD,\n",
    "       - 'rmsprop': RMSprop,\n",
    "       - 'adagrad': Adagrad,\n",
    "       - 'adadelta': Adadelta,\n",
    "       - 'adam': Adam,\n",
    "       - 'adamax': Adamax,\n",
    "       - 'nadam': Nadam,\n",
    "       - 'tfoptimizer': TFOptimizer,\n",
    "\n",
    " - Learning Rate\n",
    " - Momentum, Decay and other respective parameters\n",
    " \n",
    " \n",
    "### Metrics\n",
    "\n",
    "keras/metrices.py\n",
    "\n",
    " - Accuracy\n",
    " - Precision, Recall, Fmeasure (For imbalanced classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer=RMSprop(), metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Visually showing the graph generated\n",
    "# Note .. for this section you need to use THEANO backend\n",
    "\n",
    "from theano.printing import pydotprint\n",
    "from IPython.display import Image\n",
    "\n",
    "import theano\n",
    "from keras import backend as K\n",
    "\n",
    "# Note: Keras traing function is a bit more complex than this .. with optimizers, loss & metics functions etc.\n",
    "# you can get it at model.train_function  .. you can also see model.test_function and model.predict_function\n",
    "# also they are created once you call model.fit()\n",
    "train_func = theano.function([model.layers[0].input, K.learning_phase()], model.layers[-1].output)\n",
    "\n",
    "pydotprint(train_func, './mlp.png')\n",
    "Image('./mlp.png', width='100%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import theano.d3viz as d3v\n",
    "from IPython.display import IFrame\n",
    "\n",
    "d3v.d3viz(train_func, 'mlp.html')\n",
    "IFrame('mlp.html', width=900, height=800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=nb_epoch, verbose=1, validation_data=(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "score = model.evaluate(X_test, Y_test, verbose=0)\n",
    "print('Test score:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   ###  Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Y_pred = model.predict(X_test, verbose=0)\n",
    "y_pred =  np.argmax(Y_pred, axis=1)\n",
    "\n",
    "print Y_pred.shape\n",
    "print y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# testing the trained model\n",
    "\n",
    "_len = 4\n",
    "nos =  np.random.randint(Y_pred.shape[0], size=_len)\n",
    "\n",
    "fig, axs = plt.subplots(1,_len)\n",
    "for idx in range(_len):\n",
    "    axs[idx].set_yticklabels([])\n",
    "    axs[idx].set_xticklabels([])\n",
    "    axs[idx].imshow(X_test[nos[idx]].reshape(28,28), cmap='gray')\n",
    "    axs[idx].set_title('T: {0}, P:{1}'.format(y_test[nos[idx]], y_pred[nos[idx]]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Looking into the mis-predictions\n",
    "\n",
    "comp = np.equal(y_pred, y_test)\n",
    "mis_match = list(np.where(comp==False)[0])\n",
    "\n",
    "print len(mis_match)\n",
    "#print mis_match\n",
    "\n",
    "_len = 4\n",
    "nos =  np.random.randint(len(mis_match), size=_len)\n",
    "#print nos\n",
    "\n",
    "fig, axs = plt.subplots(1,_len)\n",
    "for idx in range(_len):\n",
    "    axs[idx].set_yticklabels([])\n",
    "    axs[idx].set_xticklabels([])\n",
    "    axs[idx].imshow(X_test[mis_match[nos[idx]]].reshape(28,28), cmap='gray')\n",
    "    axs[idx].set_title('T: {0}, P:{1}'.format(y_test[mis_match[nos[idx]]], y_pred[mis_match[nos[idx]]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Trying out user input  .. Open new_inp.jpg in an image editor and modify it to check the predictions\n",
    "\n",
    "import cv2\n",
    "image = cv2.imread('new_inp.jpg')\n",
    "gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "inp_image = cv2.resize(gray_image, (28,28)).reshape(1, 784)/255.\n",
    "\n",
    "print image.shape\n",
    "print gray_image.shape\n",
    "print inp_image.shape\n",
    "plt.imshow(inp_image.reshape(28,28), cmap='gray')\n",
    "\n",
    "y_out = model.predict(inp_image)\n",
    "print '\\n\\n{0}'.format(np.argmax(y_out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, SpatialDropout2D\n",
    "from keras.layers import Convolution2D, MaxPooling2D\n",
    "from keras.utils import np_utils\n",
    "from keras.callbacks import TensorBoard, ModelCheckpoint, EarlyStopping\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "nb_classes = 10\n",
    "nb_epoch = 20\n",
    "\n",
    "# input image dimensions\n",
    "img_rows, img_cols = 28, 28\n",
    "# number of convolutional filters to use\n",
    "nb_filters = 32\n",
    "# size of pooling area for max pooling\n",
    "pool_size = (2, 2)\n",
    "# convolution kernel size\n",
    "kernel_size = (3, 3)\n",
    "\n",
    "# the data, shuffled and split between train and test sets\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "if K.image_dim_ordering() == 'th':\n",
    "    X_train = X_train.reshape(X_train.shape[0], 1, img_rows, img_cols)\n",
    "    X_test = X_test.reshape(X_test.shape[0], 1, img_rows, img_cols)\n",
    "    input_shape = (1, img_rows, img_cols)\n",
    "else:\n",
    "    X_train = X_train.reshape(X_train.shape[0], img_rows, img_cols, 1)\n",
    "    X_test = X_test.reshape(X_test.shape[0], img_rows, img_cols, 1)\n",
    "    input_shape = (img_rows, img_cols, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print X_train.shape, y_train.shape\n",
    "print X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "Y_train = np_utils.to_categorical(y_train, nb_classes)\n",
    "Y_test = np_utils.to_categorical(y_test, nb_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# define two groups of layers: feature (convolutions) and classification (dense)\n",
    "feature_layers = [\n",
    "    Convolution2D(nb_filters, kernel_size[0], kernel_size[1],\n",
    "                  border_mode='valid', activation = 'relu',\n",
    "                  input_shape=input_shape, subsample=pool_size),\n",
    "    SpatialDropout2D(0.25),\n",
    "    Convolution2D(nb_filters, kernel_size[0], kernel_size[1], activation = 'relu', subsample=pool_size),\n",
    "    SpatialDropout2D(0.25),\n",
    "    Flatten(),\n",
    "]\n",
    "\n",
    "classification_layers = [\n",
    "    Dense(128),\n",
    "    Activation('relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(nb_classes),\n",
    "    Activation('softmax')\n",
    "]\n",
    "\n",
    "# create complete model\n",
    "model = Sequential(feature_layers + classification_layers)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see the # of parameters decreased significantly .. by using convolution layers.\n",
    "\n",
    "Even here the main culprits are the dense layers almost 94% of the parameters are contributed by the dense layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Callbacks\n",
    "\n",
    "keras/callbacks.py\n",
    " - ModelCheckpoint\n",
    " - EarlyStopping\n",
    " - TensorBoard\n",
    " - LambdaCallback .. or better extend keras.callbacks.Callback: Eg: class customCallback(keras.callbacks.Callback)\n",
    "\n",
    "should checkout: keras/engine/trainining.py ... def fit() and def _fit_loop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop',metrics=['accuracy'])\n",
    "\n",
    "\n",
    "import shutil\n",
    "try:\n",
    "    shutil.rmtree('./logs/')\n",
    "except:\n",
    "    pass\n",
    "\n",
    "\n",
    "# Here I am demonstrating tensorboard visualization ..\n",
    "# Must use tenserflow backend for this.\n",
    "\n",
    "# tensorboard --logdir=~/0.Work/0.TA_work/DL-keras/logs   ## change the log directory accordingly\n",
    "tb = TensorBoard(log_dir='./logs', write_images=True)\n",
    "\n",
    "# by default it works on 'val_loss'(monitor) .. whenever it gets better this checkpoint saves the model\n",
    "mc = ModelCheckpoint(filepath='model.h5', verbose=1, save_best_only=True)\n",
    "\n",
    "# if validation accuracy dont get improve for 5(patience) epoches the training stops \n",
    "es = EarlyStopping(monitor='val_acc', patience=5, verbose=1, mode='auto')\n",
    "\n",
    "model.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=nb_epoch,\n",
    "          verbose=1, validation_data=(X_test, Y_test), callbacks=[tb, mc, es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# You can use the previous model tests to test the new models .. \n",
    "#    other than the user input one.. as the shape of input in different in this model.\n",
    "\n",
    "import cv2\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "image = cv2.imread('new_inp.jpg')\n",
    "gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "if K.image_dim_ordering() == 'th':\n",
    "    inp_image = cv2.resize(gray_image, (28,28)).reshape(1, 1, 28, 28)/255.\n",
    "else:\n",
    "    inp_image = cv2.resize(gray_image, (28,28)).reshape(1, 28, 28, 1)/255.\n",
    "\n",
    "\n",
    "print image.shape\n",
    "print gray_image.shape\n",
    "print inp_image.shape\n",
    "plt.imshow(inp_image.reshape(28,28), cmap='gray')\n",
    "\n",
    "y_out = model.predict(inp_image, batch_size=1)\n",
    "print '\\n\\n{0}'.format(np.argmax(y_out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Save-Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save('model.h5')\n",
    "model.save_weights('model_weights.h5')\n",
    "\n",
    "\n",
    "model.load('model.h5')\n",
    "model.load_weights('model_weights.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Officially supported .. see example: keras/examples/mnist_sklear_wrapper.py\n",
    "\n",
    "\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "\n",
    "dense_size_candidates = [[32], [64], [32, 32], [64, 64]]\n",
    "my_classifier = KerasClassifier(model, batch_size=32)\n",
    "validator = GridSearchCV(my_classifier,\n",
    "                         param_grid={'dense_layer_sizes': dense_size_candidates,\n",
    "                                     # nb_epoch is avail for tuning even when not\n",
    "                                     # an argument to model building function\n",
    "                                     'nb_epoch': [3, 6],\n",
    "                                     'nb_filters': [8],\n",
    "                                     'nb_conv': [3],\n",
    "                                     'nb_pool': [2]},\n",
    "                         scoring='log_loss',\n",
    "                         n_jobs=1)\n",
    "validator.fit(X_train, y_train)\n",
    "\n",
    "print('The parameters of the best model are: ')\n",
    "print(validator.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## https://github.com/maxpumperla/hyperas\n",
    "\n",
    "from hyperopt import Trials, STATUS_OK, tpe\n",
    "from hyperas import optim\n",
    "from hyperas.distributions import choice, uniform, conditional\n",
    "\n",
    "### Other distributions:\n",
    "    # randint\n",
    "    # loguniform\n",
    "    # normal\n",
    "    # lognormal\n",
    "\n",
    "\n",
    "def data():\n",
    "    (X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "    X_train = X_train.reshape(60000, 784)\n",
    "    X_test = X_test.reshape(10000, 784)\n",
    "    X_train = X_train.astype('float32')\n",
    "    X_test = X_test.astype('float32')\n",
    "    X_train /= 255\n",
    "    X_test /= 255\n",
    "    nb_classes = 10\n",
    "    Y_train = np_utils.to_categorical(y_train, nb_classes)\n",
    "    Y_test = np_utils.to_categorical(y_test, nb_classes)\n",
    "    return X_train, Y_train, X_test, Y_test\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def model(X_train, Y_train, X_test, Y_test):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(512, input_shape=(784,)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout({{uniform(0, 1)}}))\n",
    "    model.add(Dense({{choice([256, 512, 1024])}}))\n",
    "    model.add(Activation({{choice(['relu', 'sigmoid'])}}))\n",
    "    model.add(Dropout({{uniform(0, 1)}}))\n",
    "\n",
    "    # If we choose 'four', add an additional fourth layer\n",
    "    if conditional({{choice(['three', 'four'])}}) == 'four':\n",
    "        model.add(Dense(100))\n",
    "        # We can also choose between complete sets of layers\n",
    "        model.add({{choice([Dropout(0.5), Activation('linear')])}})\n",
    "        model.add(Activation('relu'))\n",
    "\n",
    "    model.add(Dense(10))\n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', metrics=['accuracy'],\n",
    "                  optimizer={{choice(['rmsprop', 'adam', 'sgd'])}})\n",
    "\n",
    "    model.fit(X_train, Y_train,\n",
    "              batch_size={{choice([64, 128])}},\n",
    "              nb_epoch=1,\n",
    "              show_accuracy=True,\n",
    "              verbose=2,\n",
    "              validation_data=(X_test, Y_test))\n",
    "    score, acc = model.evaluate(X_test, Y_test, verbose=0)\n",
    "    print('Test accuracy:', acc)\n",
    "    return {'loss': -acc, 'status': STATUS_OK, 'model': model}\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    best_run, best_model = optim.minimize(model=model,\n",
    "                                          data=data,\n",
    "                                          algo=tpe.suggest,\n",
    "                                          max_evals=5,\n",
    "                                          trials=Trials())\n",
    "    X_train, Y_train, X_test, Y_test = data()\n",
    "    print(\"Evalutation of best performing model:\")\n",
    "    print(best_model.evaluate(X_test, Y_test))\n",
    "    \n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
